{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6263dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d979945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_station_name        day  n_pickups  n_dropoffs  net_flow  year  month  \\\n",
      "0   1 Ave & E 110 St 2022-12-28          0           0         0  2022     12   \n",
      "1   1 Ave & E 110 St 2022-12-29          0           0         0  2022     12   \n",
      "2   1 Ave & E 110 St 2022-12-30          0           0         0  2022     12   \n",
      "3   1 Ave & E 110 St 2022-12-31          0           0         0  2022     12   \n",
      "4   1 Ave & E 110 St 2023-01-01         31          32         1  2023      1   \n",
      "\n",
      "   day_of_month  day_of_week  is_weekend  net_flow_lag_1d  net_flow_lag_7d  \\\n",
      "0            28            3           0             <NA>             <NA>   \n",
      "1            29            4           0                0             <NA>   \n",
      "2            30            5           0                0             <NA>   \n",
      "3            31            6           1                0             <NA>   \n",
      "4             1            0           1                0             <NA>   \n",
      "\n",
      "   net_flow_mean_7d  \n",
      "0               NaN  \n",
      "1               0.0  \n",
      "2               0.0  \n",
      "3               0.0  \n",
      "4               0.0  \n",
      "(2434377, 13)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Paths – adjust as needed\n",
    "# -------------------------------------------------------------------\n",
    "TRIPS_PATH = \"../data/processed/citibike/*/*/data.parquet\"\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Base CTEs: trips, pickups, dropoffs, dense station × day grid\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "base_sql = f\"\"\"\n",
    "WITH trips AS (\n",
    "    SELECT\n",
    "        start_station_name AS start_station,\n",
    "        end_station_name  AS end_station,\n",
    "        date_trunc('day', started_at) AS day\n",
    "    FROM read_parquet('{TRIPS_PATH}')\n",
    "    WHERE started_at IS NOT NULL\n",
    "),\n",
    "-- All unique stations (from starts and ends)\n",
    "stations AS (\n",
    "    SELECT DISTINCT station FROM (\n",
    "        SELECT start_station AS station FROM trips\n",
    "        UNION\n",
    "        SELECT end_station   AS station FROM trips\n",
    "    )\n",
    "),\n",
    "-- Date bounds\n",
    "bounds AS (\n",
    "    SELECT\n",
    "        min(day) AS min_day,\n",
    "        max(day) AS max_day\n",
    "    FROM trips\n",
    "),\n",
    "-- All days between min and max (dense)\n",
    "days AS (\n",
    "    SELECT\n",
    "        min_day + (i || ' days')::INTERVAL AS day\n",
    "    FROM bounds,\n",
    "         range(date_diff('day', min_day, max_day) + 1) AS t(i)\n",
    "),\n",
    "-- Dense station × day grid\n",
    "grid AS (\n",
    "    SELECT\n",
    "        s.station,\n",
    "        d.day\n",
    "    FROM stations s\n",
    "    CROSS JOIN days d\n",
    "),\n",
    "-- Daily pickups per station\n",
    "pickups AS (\n",
    "    SELECT\n",
    "        start_station AS station,\n",
    "        day,\n",
    "        COUNT(*) AS n_pickups\n",
    "    FROM trips\n",
    "    GROUP BY start_station, day\n",
    "),\n",
    "-- Daily dropoffs per station\n",
    "dropoffs AS (\n",
    "    SELECT\n",
    "        end_station AS station,\n",
    "        day,\n",
    "        COUNT(*) AS n_dropoffs\n",
    "    FROM trips\n",
    "    GROUP BY end_station, day\n",
    "),\n",
    "-- Combine grid with pickups & dropoffs\n",
    "daily_base AS (\n",
    "    SELECT\n",
    "        g.station AS start_station_name,\n",
    "        g.day,\n",
    "        COALESCE(p.n_pickups, 0)  AS n_pickups,\n",
    "        COALESCE(d.n_dropoffs, 0) AS n_dropoffs,\n",
    "        COALESCE(d.n_dropoffs, 0) - COALESCE(p.n_pickups, 0) AS net_flow\n",
    "    FROM grid g\n",
    "    LEFT JOIN pickups  p USING (station, day)\n",
    "    LEFT JOIN dropoffs d USING (station, day)\n",
    "),\n",
    "-- Add time features\n",
    "daily_with_time AS (\n",
    "    SELECT\n",
    "        start_station_name,\n",
    "        day,\n",
    "        n_pickups,\n",
    "        n_dropoffs,\n",
    "        net_flow,\n",
    "        CAST(strftime(day, '%Y') AS INTEGER) AS year,\n",
    "        CAST(strftime(day, '%m') AS INTEGER) AS month,\n",
    "        CAST(strftime(day, '%d') AS INTEGER) AS day_of_month,\n",
    "        CAST(strftime(day, '%w') AS INTEGER) AS day_of_week,   -- 0=Sunday,...,6=Saturday\n",
    "        CASE\n",
    "            WHEN CAST(strftime(day, '%w') AS INTEGER) IN (0, 6) THEN 1 ELSE 0\n",
    "        END AS is_weekend\n",
    "    FROM daily_base\n",
    "),\n",
    "-- Add lag & rolling features (by station)\n",
    "daily_features AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        -- 1-day lag of net flow\n",
    "        LAG(net_flow, 1) OVER (\n",
    "            PARTITION BY start_station_name\n",
    "            ORDER BY day\n",
    "        ) AS net_flow_lag_1d,\n",
    "\n",
    "        -- 7-day lag of net flow\n",
    "        LAG(net_flow, 7) OVER (\n",
    "            PARTITION BY start_station_name\n",
    "            ORDER BY day\n",
    "        ) AS net_flow_lag_7d,\n",
    "\n",
    "        -- rolling mean net flow over last 7 days (excluding today)\n",
    "        AVG(net_flow) OVER (\n",
    "            PARTITION BY start_station_name\n",
    "            ORDER BY day\n",
    "            ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING\n",
    "        ) AS net_flow_mean_7d\n",
    "    FROM daily_with_time\n",
    ")\n",
    "SELECT * FROM daily_features\n",
    "ORDER BY start_station_name, day\n",
    "\"\"\"\n",
    "\n",
    "df_daily = con.execute(base_sql).fetchdf()\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(df_daily.head())\n",
    "print(df_daily.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d4de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_cols = [\"net_flow_lag_1d\", \"net_flow_lag_7d\", \"net_flow_mean_7d\"]\n",
    "df_model = df_daily.dropna(subset=lag_cols).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feadbfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129787 215556 72633\n"
     ]
    }
   ],
   "source": [
    "# df_model[\"start_station_name\"] = df_model[\"start_station_name\"].astype(\"category\").cat.codes\n",
    "train = df_model[df_model[\"day\"] <  \"2025-07-01\"]\n",
    "val   = df_model[(df_model[\"day\"] >= \"2025-07-01\") & (df_model[\"day\"] < \"2025-10-01\")]\n",
    "test  = df_model[df_model[\"day\"] >= \"2025-10-01\"]\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad01fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/gmnjxh5x1hn48qy93ptqpz580000gn/T/ipykernel_24953/743469052.py:20: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  X_train = pd.get_dummies(train[feature_cols], columns=[\"start_station_name\"], sparse=True).values\n",
      "/var/folders/6c/gmnjxh5x1hn48qy93ptqpz580000gn/T/ipykernel_24953/743469052.py:23: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  X_val   = pd.get_dummies(val[feature_cols], columns=[\"start_station_name\"], sparse=True).values\n",
      "/var/folders/6c/gmnjxh5x1hn48qy93ptqpz580000gn/T/ipykernel_24953/743469052.py:26: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  X_test  = pd.get_dummies(test[feature_cols], columns=[\"start_station_name\"], sparse=True).values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_cols = [\n",
    "    \"start_station_name\",\n",
    "    \"day_of_week\",\n",
    "    \"is_weekend\",\n",
    "    \"month\",\n",
    "    \"net_flow_lag_1d\",\n",
    "    \"net_flow_lag_7d\",\n",
    "    \"net_flow_mean_7d\",\n",
    "]\n",
    "\n",
    "# X_train = train[feature_cols]\n",
    "# y_train = train[\"net_flow\"]\n",
    "\n",
    "# X_val   = val[feature_cols]\n",
    "# y_val   = val[\"net_flow\"]\n",
    "\n",
    "# X_test  = test[feature_cols]\n",
    "# y_test  = test[\"net_flow\"]\n",
    "\n",
    "X_train = pd.get_dummies(train[feature_cols], columns=[\"start_station_name\"], sparse=True).values\n",
    "y_train = train[\"net_flow\"]\n",
    "\n",
    "X_val   = pd.get_dummies(val[feature_cols], columns=[\"start_station_name\"], sparse=True).values\n",
    "y_val   = val[\"net_flow\"]\n",
    "\n",
    "X_test  = pd.get_dummies(test[feature_cols], columns=[\"start_station_name\"], sparse=True).values\n",
    "y_test  = test[\"net_flow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "model = HistGradientBoostingRegressor(\n",
    "    max_iter=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    # categorical_features=[\"start_station_name\"],\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e61c45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jesper/Documents/AXA/DSC/.conda/lib/python3.11/site-packages/xgboost/data.py:399: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "/Users/jesper/Documents/AXA/DSC/.conda/lib/python3.11/site-packages/xgboost/data.py:399: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "/Users/jesper/Documents/AXA/DSC/.conda/lib/python3.11/site-packages/xgboost/data.py:399: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:6.31780\tval-rmse:7.48673\n",
      "[50]\ttrain-rmse:5.91194\tval-rmse:7.12684\n",
      "[99]\ttrain-rmse:5.86905\tval-rmse:7.09979\n",
      "Test MAE (net_flow):  4.091\n",
      "Test RMSE (net_flow): 7.048\n",
      "Test WMAPE:           97.111%\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval   = xgb.DMatrix(X_val,   label=y_val)\n",
    "dtest  = xgb.DMatrix(X_test,  label=y_test)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",   # net_flow can be negative, so plain regression\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"max_depth\": 8,\n",
    "    \"eta\": 0.1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "}\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dval, \"val\")]\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=100,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50,\n",
    ")\n",
    "\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "wmape = np.abs(y_pred - y_test).sum() / np.abs(y_test).sum()\n",
    "\n",
    "print(f\"Test MAE (net_flow):  {mae:.3f}\")\n",
    "print(f\"Test RMSE (net_flow): {rmse:.3f}\")\n",
    "print(f\"Test WMAPE:           {wmape:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb15e2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daily[\"net_flow\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4fcad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
